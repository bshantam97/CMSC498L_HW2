{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CMSC 498L: Introduction to Deep Learning- HW3\n",
    "\n",
    "#### Name: Shantam Bajpai\n",
    "#### UID: 116831956\n",
    "\n",
    "\n",
    "## Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "from torchvision import datasets,transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "from scipy import ndimage\n",
    "from PIL import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset and convert to PyTorch Tensors\n",
    "\n",
    "Below we load our training and testing data and convert the input and the output labels to pytorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_file, test_file):\n",
    "    # Load the training data\n",
    "    train_dataset = h5py.File(train_file,'r')\n",
    "    \n",
    "    # Separate features(x) and labels(y) for training set\n",
    "    train_set_x_orig = np.array(train_dataset['train_set_x'])\n",
    "    train_set_y_orig = np.array(train_dataset['train_set_y'])\n",
    "\n",
    "    # Load the test data\n",
    "    test_dataset = h5py.File(test_file,'r')\n",
    "    \n",
    "    # Separate features(x) and labels(y) for training set\n",
    "    test_set_x_orig = np.array(test_dataset['test_set_x'])\n",
    "    test_set_y_orig = np.array(test_dataset['test_set_y'])\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",
    "\n",
    "train_file=\"data/train_catvnoncat.h5\"\n",
    "test_file=\"data/test_catvnoncat.h5\"\n",
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data(train_file, test_file)\n",
    "\n",
    "# Reshape the training and testing examples\n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1)   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1)\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "# Convert the input and output to PyTorch Tensors\n",
    "\n",
    "train_x = torch.from_numpy(train_x).float()\n",
    "test_x = torch.from_numpy(test_x).float()\n",
    "train_y = torch.from_numpy(train_y).float()\n",
    "test_y = torch.from_numpy(test_y).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class for the Network\n",
    "\n",
    "Below is the implementation for the 2 layer neural network and the subsequent initialization for the number of input neurons, the number of hidden neurons and the neurons in the output layer.\n",
    "\n",
    "The first layer is passed through a ReLu activation unit followed by sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNetwork(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    n_x: The number of neurons in the input layer\n",
    "    n_h: The number of neurons in the hidden layer\n",
    "    n_y: The number of neurons in the output layer\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,n_x = 12288, n_h = 7, n_y = 1):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(n_x,n_h)\n",
    "        self.fc2 = nn.Linear(n_h,n_y)\n",
    "    \n",
    "    \"\"\"\n",
    "    Function for computing the forward pass with a relu activation unit in fully connected layer 1\n",
    "    and returns an output from the sigmoid activation unit\n",
    "    \"\"\"\n",
    "    def forward(self, X):\n",
    "        \n",
    "        X = f.relu(self.fc1(X))\n",
    "        X = self.fc2(X)\n",
    "        \n",
    "        return torch.sigmoid(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of TwoLayerNetwork(\n",
       "  (fc1): Linear(in_features=12288, out_features=7, bias=True)\n",
       "  (fc2): Linear(in_features=7, out_features=1, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set a random seed to random initilization of the weights and biases\n",
    "torch.manual_seed(101)\n",
    "\n",
    "# Create an object of the network\n",
    "model = TwoLayerNetwork()\n",
    "\n",
    "# Print out the model parameters\n",
    "model.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here tried using Cross Entropy loss for the binary classification problem but it didnot work. After some brainstorming and researching on stackoverflow and pytorch forums, BCELoss() worked perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define our optimizer as a Cross Entropy Loss Function \n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "\n",
    "# We define our criterion to be Gradient Descent \n",
    "#torch.optim.SGD(model.parameters(),lr = 0.01,momentum=0.9, weight_decay=0.001)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shant\\anaconda3\\envs\\env\\lib\\site-packages\\torch\\nn\\modules\\loss.py:498: UserWarning: Using a target size (torch.Size([1, 209])) that is different to the input size (torch.Size([209, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :0 loss:0.7448181509971619\n",
      "epoch :100 loss:0.371491014957428\n",
      "epoch :200 loss:0.14398454129695892\n",
      "epoch :300 loss:0.05015343055129051\n",
      "epoch :400 loss:0.02215004153549671\n",
      "epoch :500 loss:0.012089536525309086\n",
      "epoch :600 loss:0.007572536822408438\n",
      "epoch :700 loss:0.005184698849916458\n",
      "epoch :800 loss:0.0037714517675340176\n",
      "epoch :900 loss:0.0028652031905949116\n",
      "epoch :1000 loss:0.002248611766844988\n",
      "epoch :1100 loss:0.0018097207648679614\n",
      "epoch :1200 loss:0.0014859653310850263\n",
      "epoch :1300 loss:0.0012401430867612362\n",
      "epoch :1400 loss:0.0010490057757124305\n",
      "epoch :1500 loss:0.0008974351221695542\n",
      "epoch :1600 loss:0.000775194144807756\n",
      "epoch :1700 loss:0.0006751795881427824\n",
      "epoch :1800 loss:0.0005923277349211276\n",
      "epoch :1900 loss:0.0005229392554610968\n",
      "epoch :2000 loss:0.00046425912296399474\n",
      "epoch :2100 loss:0.00041420882917009294\n",
      "epoch :2200 loss:0.0003711914469022304\n",
      "epoch :2300 loss:0.000333966949256137\n",
      "epoch :2400 loss:0.00030155619606375694\n",
      "epoch :2500 loss:0.00027316881460137665\n",
      "epoch :2600 loss:0.0002481841656845063\n",
      "epoch :2700 loss:0.00022609505685977638\n",
      "epoch :2800 loss:0.0002064785803668201\n",
      "epoch :2900 loss:0.00018898415146395564\n",
      "Duration: 0.15071853399276733mins\n"
     ]
    }
   ],
   "source": [
    "# Training our model\n",
    "import time\n",
    "\n",
    "# Set the start time for model execution\n",
    "start_time  = time.time()\n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 3000\n",
    "\n",
    "# Trackers\n",
    "train_correct = [] # What we get correct during training phase\n",
    "train_losses = []  # What we get wrong during the training phase\n",
    "test_correct = []  # what we get correct during the testing phase\n",
    "test_losses = []   # What we get wrong during the testing phase\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    # Predicted Labels\n",
    "    y_prediction = model(train_x)\n",
    "    \n",
    "    # Computed Loss using cross entropy\n",
    "    loss = criterion(y_prediction,train_y)\n",
    "    \n",
    "    # Append the losses in a list\n",
    "    train_losses.append(loss)\n",
    "    \n",
    "    if i%100 == 0:\n",
    "        \n",
    "        print(f'epoch :{i} loss:{loss.item()}')\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "# Calculates the time elapsed\n",
    "total_time = time.time() - start_time\n",
    "print(f'Duration: {total_time/60}mins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Phase for the Classification problem\n",
    "\n",
    "In the testing phase we perform a single forward pass through the layer after training has been performed for a set number of iterations. torch.no_grad() has been used as we do not require gradient calculation while doing a single forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 70.0%\n"
     ]
    }
   ],
   "source": [
    "test_corr = 0\n",
    "y_val = model(test_x)\n",
    "# Accuracy of our prediction\n",
    "# We use torch.zero_grad top conserve memory as we do not require gradient calculation while running our model on the test data\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for i in range(0,len(y_val)):\n",
    "        \n",
    "        # Threshold value\n",
    "        if(y_val[i] > 0.5):\n",
    "            y_val[i] = 1\n",
    "        else:\n",
    "            y_val[i] = 0\n",
    "    \n",
    "        # Test correct\n",
    "        test_corr += (y_val[i]==test_y[0,i]).sum()\n",
    "print(f'Testing Accuracy: {100*test_corr.item() / (len(y_val))}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters tested and the optimal Hyperparameters found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning rate: 0.01 iterations: 4000 loss: 0.0052 momentum: 0 testing accuracy: 78%\n",
    "\n",
    "learning rate: 0.01 iterations: 4000 loss: 0.00345 momentum: 0.9 testing accuracy: 70%\n",
    "\n",
    "learning rate: 0.01 iterations: 5000 loss: 0.00262 momentum: 0.9 testing accuracy: 70%\n",
    "\n",
    "learning rate: 0.001 iterations: 5000 loss: 0.00411 momentum: 0.9 testing accuracy: 72%\n",
    "\n",
    "learning rate: 0.01 iterations: 5000 loss: 0.64829 momentum: 0.9 weight decay: 0.1 testing accuracy: 34%\n",
    "\n",
    "learning rate: 0.1 iterations: 5000 loss: 0.6439 momentum: 0.9 weight decay: 0.1 testing accuracy: 34%\n",
    "\n",
    "learning rate: 0.1 iterations: 4000 loss: 0.64397 momentum: 0.9 weight decay: 0 testing accuracy: 34%\n",
    "\n",
    "learning rate: 0.01 iterations: 7000 loss: 0.001838 momentum: 0.9 testing accuracy: 70%\n",
    "\n",
    "learning rate: 0.001 iterations: 8000 loss: 0.00204 momentum: 0.9 testing accuracy: 72%\n",
    "\n",
    "learning rate: 0.05 iterations: 8000 loss: 0.64397 momentum: 0.9 testing accuracy: 34%\n",
    "\n",
    "learning rate: 0.5 iterations: 5000 loss: 18.1122 momentum: 0.9 testing accuracy: 66%\n",
    "\n",
    "learning rate: 0.01 iterations: 5000 loss: 18.0321 momentum: 0.9 weight decay:0.1 testing accuracy: 34%\n",
    "\n",
    "learning rate: 0.01 iterations: 5000 loss: 0.0322 momentum: 0.99 testing accuracy: 58%\n",
    "\n",
    "learning rate: 0.01 iterations: 5000 loss: 0.01227 momentum: 0.99 weight decay: 0.01 testing accuracy: 74%\n",
    "\n",
    "learning rate: 0.01 iterations: 5000 loss: 0.012275 momentum: 0 weight decay:0.01 testing accuracy: 74%\n",
    "\n",
    "### The best Hyperparameter Values found for the Classification problem\n",
    "\n",
    "**learning rate: 0.01 iterations: 4000 loss: 0.0052 momentum: 0 testing accuracy: 78%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "\n",
    "The first part for implementing sentiment analysis is importing the dataset and and then preprocessing the noisy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_file, test_file):\n",
    "    train_dataset = []\n",
    "    test_dataset = []\n",
    "    \n",
    "    # Read the training dataset file line by line\n",
    "    for line in open(train_file, 'r',encoding = \"utf8\"):\n",
    "        train_dataset.append(line.strip())\n",
    "        \n",
    "    for line in open(test_file, 'r',encoding=\"utf8\"):\n",
    "        test_dataset.append(line.strip())\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process Noisy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_reviews(reviews):\n",
    "    \n",
    "    reviews = [REPLACE_NO_SPACE.sub(NO_SPACE, line.lower()) for line in reviews]\n",
    "    reviews = [REPLACE_WITH_SPACE.sub(SPACE, line) for line in reviews]\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "# Load the train and the test files\n",
    "train_file = \"data/train_imdb.txt\"\n",
    "test_file = \"data/test_imdb.txt\"\n",
    "train_dataset, test_dataset = load_data(train_file, test_file)\n",
    "\n",
    "# This is just how the data is organized. The first 50% data is positive and the rest 50% is negative for both train and test splits.\n",
    "y = [1 if i < len(train_dataset)*0.5 else 0 for i in range(len(train_dataset))]\n",
    "\n",
    "# Preprocessing\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "NO_SPACE = \"\"\n",
    "SPACE = \" \"\n",
    "\n",
    "# Pre-Processing\n",
    "train_dataset_clean = preprocess_reviews(train_dataset)\n",
    "test_dataset_clean = preprocess_reviews(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(binary=True, stop_words=\"english\", max_features=2000)\n",
    "cv.fit(train_dataset_clean)\n",
    "X = cv.transform(train_dataset_clean)\n",
    "X_test = cv.transform(test_dataset_clean)\n",
    "X = np.array(X.todense()).astype(float)\n",
    "X_test = np.array(X_test.todense()).astype(float)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size = 0.80)\n",
    "\n",
    "# This is just to correct the shape of the arrays as required by the two_layer_model\n",
    "# In PyTorch the batch size comes first\n",
    "y_train = y_train.reshape(1,-1)\n",
    "y_val = y_val.reshape(1,-1)\n",
    "\n",
    "# Convert to Pytorch tensors\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train).float()\n",
    "X_val = torch.from_numpy(X_val).float()\n",
    "y_val = torch.from_numpy(y_val).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 800])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([201, 2000])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Layer Class Model\n",
    "\n",
    "Below is the 2 layer neural network to perform sentiment analysis on the dataset. Here n_x is the number of input neurons, n_h is the number of hidden neurons and n_y is the number of output neurons(As this is a binary classification problem output neuron will be 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNetworkSentiment(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    n_x: The number of neurons in the input layer\n",
    "    n_h: The number of neurons in the hidden layer\n",
    "    n_y: The number of neurons in the output layer\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,n_x = X_train.shape[1], n_h = 200, n_y = 1):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(n_x,n_h)\n",
    "        self.fc2 = nn.Linear(n_h,n_y)\n",
    "    \n",
    "    \"\"\"\n",
    "    Function for computing the forward pass with a relu activation unit in fully connected layer 1\n",
    "    and returns an output from the sigmoid activation unit\n",
    "    \"\"\"\n",
    "    def forward(self, X):\n",
    "        \n",
    "        X = f.relu(self.fc1(X))\n",
    "        X = self.fc2(X)\n",
    "        \n",
    "        return torch.sigmoid(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of TwoLayerNetworkSentiment(\n",
       "  (fc1): Linear(in_features=2000, out_features=200, bias=True)\n",
       "  (fc2): Linear(in_features=200, out_features=1, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set a random seed to random initilization of the weights and biases\n",
    "torch.manual_seed(101)\n",
    "\n",
    "# Create an object of the network\n",
    "Sentiment = TwoLayerNetworkSentiment()\n",
    "\n",
    "# Print out the model parameters\n",
    "Sentiment.parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criterion and Optimizer\n",
    "\n",
    "The criterion that I have used here is the Binary Cross Entropy Loss function and the optimizer that I have used is the Stochastic Gradient Descent Optimizer in which I will be vary =ing the learning rate, Momentum value and L2 Penalty(weight decay) to obtain the highest test dataset prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define our optimizer as a Cross Entropy Loss Function \n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# We define our criterion to be Gradient Descent \n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = 0.01,momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :0 loss:0.6935747265815735\n",
      "epoch :100 loss:0.6935747265815735\n",
      "epoch :200 loss:0.6935747265815735\n",
      "epoch :300 loss:0.6935747265815735\n",
      "epoch :400 loss:0.6935747265815735\n",
      "epoch :500 loss:0.6935747265815735\n",
      "epoch :600 loss:0.6935747265815735\n",
      "epoch :700 loss:0.6935747265815735\n",
      "epoch :800 loss:0.6935747265815735\n",
      "epoch :900 loss:0.6935747265815735\n",
      "epoch :1000 loss:0.6935747265815735\n",
      "epoch :1100 loss:0.6935747265815735\n",
      "epoch :1200 loss:0.6935747265815735\n",
      "epoch :1300 loss:0.6935747265815735\n",
      "epoch :1400 loss:0.6935747265815735\n",
      "epoch :1500 loss:0.6935747265815735\n",
      "epoch :1600 loss:0.6935747265815735\n",
      "epoch :1700 loss:0.6935747265815735\n",
      "epoch :1800 loss:0.6935747265815735\n",
      "epoch :1900 loss:0.6935747265815735\n",
      "epoch :2000 loss:0.6935747265815735\n",
      "epoch :2100 loss:0.6935747265815735\n",
      "epoch :2200 loss:0.6935747265815735\n",
      "epoch :2300 loss:0.6935747265815735\n",
      "epoch :2400 loss:0.6935747265815735\n",
      "epoch :2500 loss:0.6935747265815735\n",
      "epoch :2600 loss:0.6935747265815735\n",
      "epoch :2700 loss:0.6935747265815735\n",
      "epoch :2800 loss:0.6935747265815735\n",
      "epoch :2900 loss:0.6935747265815735\n",
      "Duration: 0.5028387268384298mins\n"
     ]
    }
   ],
   "source": [
    "# Training our model\n",
    "import time\n",
    "\n",
    "# Set the start time for model execution\n",
    "start_time  = time.time()\n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 3000\n",
    "\n",
    "# Trackers\n",
    "train_losses = []  # What we get wrong during the training phase\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    # Predicted Labels\n",
    "    y_prediction = Sentiment(X_train)\n",
    "    \n",
    "    # Computed Loss using cross entropy\n",
    "    loss = criterion(y_prediction,y_train)\n",
    "    \n",
    "    # Append the losses in a list\n",
    "    train_losses.append(loss)\n",
    "    \n",
    "    if i%100 == 0:\n",
    "        \n",
    "        print(f'epoch :{i} loss:{loss.item()}')\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "# Calculates the time elapsed\n",
    "total_time = time.time() - start_time\n",
    "print(f'Duration: {total_time/60}mins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 56.21890547263681%\n"
     ]
    }
   ],
   "source": [
    "test_corr = 0\n",
    "y_eval = Sentiment(X_val)\n",
    "# Accuracy of our prediction\n",
    "# We use torch.zero_grad top conserve memory as we do not require gradient calculation while running our model on the test data\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for i in range(0,len(y_eval)):\n",
    "        \n",
    "        # Threshold value\n",
    "        if(y_eval[i] > 0.5):\n",
    "            y_eval[i] = 1\n",
    "        else:\n",
    "            y_eval[i] = 0\n",
    "    \n",
    "        # Test correct\n",
    "        test_corr += (y_eval[i]==y_val[0,i]).sum()\n",
    "print(f'Testing Accuracy: {100*test_corr.item() / (len(y_eval))}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Testing\n",
    "\n",
    "learning rate: 0.01 iterations: 3000 loss: 0.693967 momentum: 0 weight decay: 0 testing accuracy: 56.218%\n",
    "\n",
    "learning rate: 0.01 iterations: 3000 loss: 0.693967 momentum: 0 weight decay: 0 testing accuracy: 56.218%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
